\documentclass[11pt]{article}

% Page layout (single-column, professional margins)
\usepackage[margin=1in]{geometry}

% Fonts and text
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}

% Figures and tables
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}

% Math
\usepackage{amsmath}

% URLs
\usepackage{url}

% Title
\title{Group X Progress Report:\\Heart Disease Prediction using Machine Learning}

\author{
ZhiChong Lin, ZiDi Yao, Ke Ma\\
\texttt{yaoz25@mcmaster.ca, mak11@mcmaster.ca, lin281@mcmaster.ca}
}

\date{}
\setlength{\parindent}{2em} 

\begin{document}
\maketitle

% ============================================================
\section{Introduction}

    This project aims to use ML method to predict the likelihood
of heart disease based on eleven clinical and demographic features. Heart
disease continues to be a major global health concern, and early detection is
crucial for reducing severe outcomes. As outlined in our project proposal, the
goal of this work is to develop a reliable classification model that can
identify high-risk patients using routinely collected medical measurements.

    In this progress report, we will discuss the steps completed so far, including
dataset preprocessing, feature encoding, and the development of a Chi-Square +
PCA feature-engineering pipeline. We also present initial results from models
such as SVM, Logistic Regression, and Random Forest, and outline feedback from
the TA along with our planned next steps.

Building on these prior studies, our project adopts a supervised learning pipeline that combines Chi-square feature selection and Principal Component Analysis (PCA) with an RBF-kernel SVM classifier. 
We first apply a column transformer with median imputation and standardization for numerical features, and most-frequent imputation with one-hot encoding for categorical features;
Then we use a MinMaxScaler followed by \texttt{SelectKBest} with the Chi-square statistic to keep the top-$k$ informative features, 
and apply PCA with the number of components chosen using the Kaiser criterion~\cite{GARATEESCAMILA2020100330,kaiser1960factor}. 
The low-dimensional representation is fed into an RBF-SVM, and the full pipeline is evaluated on the Kaggle Heart Failure Prediction dataset~\cite{heartfailure_kaggle} 
using stratified $k$-fold cross validation, reporting accuracy, precision, recall, F1-score, and AUC as in previous work.


% ============================================================
\section{Related Work}

Our progress in this project builds directly on the foundation built in
Milestone01, where we considered both the clinical motivation and the technical
approaches. In that proposal, we discussed the limitations of traditional diagnostic methods and highlighted the
importance of ML models that can identify  multi-feature interactions such as age, cholesterol level, chest pain type, and 
ECG-related measurements, which observations align with prior work showing that 
heart disease prediction is well suited to ML due to its 
multivariate and non-linear nature.

As we talked about in M1, several existing studies have explored this predictive task using classical and
modern machine learning techniques. Logistic regression remains a common 
baseline method, as demonstrated by Awan \cite{musmanaslamawan_heartdisease_logistic},
who achieved approximately 85\% accuracy on the Kaggle using minimal
feature engineering. More advanced pipelines integrate supervised feature 
selection and dimensionality reduction. The Chi-Square + PCA framework proposed 
by Gárate-Escamila et al. \cite{GARATEESCAMILA2020100330} achieved up to 99\%
accuracy on multiple UCI heart disease datasets, motivating our decision in
Milestone~1 to incorporate both Chi-Square filtering and PCA into our own
preprocessing pipeline.

Moreover, foundational statistic work such as Kaiser factor analysis criterion
\cite{kaiser1960factor} provides justification for retaining only
principal components with eigenvalues greater than one, a rule we apply in our
dimensionality reduction stage. Other work has examined optimization strategies 
for medical classification models, including scalable L1-regularized training
\cite{andrew2007scalable} and multi-task predictive structure learning
\cite{Ando2005}, which highlight broader approaches to improving generalization
when datasets are small—one of the challenges noted in our proposal.

\section{Dataset and Preprocessing} 

This section will introduce the raw dataset we used, and how we clean and process it. 

\subsection{Dataset Description}

The dataset we used in this project is the \textit{Heart Failure Prediction Dataset}
published by Fedesoriano on Kaggle~\cite{heartfailure_kaggle}.  
It contains \textbf{918 patient observations} and \textbf{12 attributes}, including 
11 clinical predictor variables and one binary target label indicating the presence 
of heart disease. This dataset was designed to support research on early detection 
of cardiovascular risks, particularly heart failure, which remains one of the leading 
causes of global mortality.

The dataset includes a mixture of demographic features ( Age, Sex), 
physiological measurements (like RestingBP, Cholesterol, MaxHR), and exercise-induced 
ECG-related metrics (ExerciseAngina, Oldpeak, ST\_Slope). Those attributes show 
common risk factors used in medical diagnostics for cardiovascular disease and have 
been widely adopted in machine learning models for clinical prediction tasks.

A complete list of raw features and their corresponding descriptions is provided in 
Table~\ref{tab:rawfeatures}.


\begin{table}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Feature} & \textbf{Description} \\
\midrule
Age & Age of patient (years) \\
Sex & Biological sex (M/F) \\
ChestPainType & Chest pain type (ATA, NAP, ASY, TA) \\
RestingBP & Resting blood pressure (mm Hg) \\
Cholesterol & Serum cholesterol (mg/dL) \\
FastingBS & Fasting blood sugar (0/1) \\
RestingECG & Resting ECG results (Normal, ST, LVH) \\
MaxHR & Maximum heart rate achieved \\
ExerciseAngina & Exercise-induced angina (Y/N) \\
Oldpeak & ST depression value induced by exercise \\
ST\_Slope & Slope of ST segment (Up, Flat, Down) \\
HeartDisease & Target label (1 = disease, 0 = healthy) \\
\bottomrule
\end{tabular}
\caption{Raw dataset features.}
\label{tab:rawfeatures}
\end{table}

\subsection{Target Extraction}

The target label \texttt{HeartDisease} is a binary indicator representing whether
a patient shows signs of heart disease.  
We extract this column and converted it to integer form using:
\[
y = \texttt{df['HeartDisease'].astype(int)}.
\]
The resulting is a one-dimensional vector , which was saved as \texttt{processed/y.csv}
for all downstream training and evaluation.

\subsection{Feature Preprocessing}

The feature matrix \(X\) was constructed by removing the target column and
retaining the remaining 11 raw input attributes.
Since the dataset includes both numerical and categorical variables, several
preprocessing steps were required to convert all values into a machine-learning‐
ready numeric form.

\paragraph{Binary Encoding}

Three features, namely Sex, ExerciseAngina, and FastingBS contain only two possible values and were mapped directly to
0/1 following our preprocessing script:

\begin{itemize}
    \item \textbf{Sex:} \texttt{M} $\rightarrow$ 1,\; \texttt{F} $\rightarrow$ 0
    \item \textbf{ExerciseAngina:} \texttt{Y} $\rightarrow$ 1,\; \texttt{N} $\rightarrow$ 0
    \item \textbf{FastingBS:} preserved as integer \texttt{0/1}
\end{itemize}

\paragraph{Ordinal Mapping of Multi-Class Features}

Three categorical features contain more than two categories.
In the stored processed dataset (\texttt{X\_encoded.csv}), they were converted
to integer codes according to predefined mappings:

\[
\text{ChestPainType: } \{\texttt{ATA}, \texttt{NAP}, \texttt{ASY}, \texttt{TA}\}
\rightarrow \{0,1,2,3\},
\]

\[
\text{RestingECG: } \{\texttt{Normal}, \texttt{ST}, \texttt{LVH}\}
\rightarrow \{0,1,2\},
\]

\[
\text{ST\_Slope: } \{\texttt{Up}, \texttt{Flat}, \texttt{Down}\}
\rightarrow \{0,1,2\}.
\]

These mappings avoid string-based ambiguity and ensure that all feature columns are numeric at the preprocessing stage.

\subsection{Final Processed Dataset}

After applying the above processing , the final processed feature matrix
contains:

\[
\textbf{918 samples} \quad \times \quad \textbf{11 fully numeric features}.
\]

The cleaned dataset was saved to \texttt{processed/X\_encoded.csv}, and
the corresponding feature names were exported to
\texttt{processed/feature\_names.txt} for reproducibility.

This processed dataset serves as the input to the feature selection (Chi-square)
and dimensionality reduction (PCA) procedures described in the next section.

% ============================================================

% This section corresponds to Item 3 in the project instructions.
\section{Model Inputs (Features)}

During our data preprocessing phase, we utilize \texttt{scikit-learn} built-in function \texttt{pipeline} to transform our data before feeding into Machine Learning Model
\vspace{1em}
The dataset has both numerical and categorical features, in the previous part we mentioned that for each numerical part we standardized them, 
while for categorical part we used one-hot encoding to transform them into vectors.
\vspace{1em}
In result, we have total 22 features after preprocessing, including 6 numerical features and 16 categorical features ``(One-Hot Encoding)''.
\vspace{1em}
We then went ahead to further transform our data by using both features selection and dimensionality reduction techniques. Which is so called Chi-PCA method \cite{GARATEESCAMILA2020100330}.
\begin{itemize}
    \item \textbf{Chi-Square:}  
    Since in Mathematically, the Chi-square statistic is defined as:
    \[
    \chi^2 = \sum_{i}\sum_{j}\frac{(O_{ij} - E_{ij})^2}{E_{ij}}
    \], where \(O_{ij}\) is the observed frequency and \(E_{ij}\) is the expected frequency. 
    \vspace{1em}
    
    Therefore, It is expecting a positive value for frequency, so we use \texttt{MinMaxScaler} to scale all numerical features to [0,1] range. So that all numerical features and categorical features are non-negative.

    Then we have a Hyperparameters 'k' to select top k features with highest Chi-square statistic with respect to the target label. In here we set k=9, as we found that 9 is the best parameter after conducting grid-search.
    \item \textbf{PCA:}  
    After feature selection, we then applied Principal Component Analysis (PCA) to reduce the dimensionality of the selected features. PCA works by identifying the directions (principal components) in which the data varies the most, and projecting the data onto these directions.

    To determine the number of principal components to retain, we adopt the \textbf{Kaiser criterion} \cite{kaiser1960factor}. 
    This rule suggests keeping only components with eigenvalues greater than 1.0.

    After doing a experiments of calculating eigenvalues for every single increase of principal components, we found that the first five components have eigenvalues greater than 1.0. Thus, we decided to retain five principal components for our final feature representation.   
\end{itemize}

Hence, each patient sample is represented as a compact (data, 5) feature vector summarizing the most informative physiological and categorical characteristics. 
This final feature set is then used as input to our machine learning model.

% ============================================================
% \section{Model Implementation}
% Describe the machine learning model(s) used.

% Examples:
% \begin{itemize}
%     \item RBF-kernel Support Vector Machine (SVM)
%     \item Justification for using SVM
%     \item Hyperparameters used (C, gamma)
%     \item Training pipeline and libraries (scikit-learn)
% \end{itemize}

% This section corresponds to Item 4 of the project instructions.

\section{Model Implementation}

We have implemented a supervised learning pipeline for binary classification of heart disease presence. 
Our main model is a \textbf{Support Vector Machine (SVM)} with a \textbf{Radial Basis Function (RBF)} kernel, implemented using the \texttt{scikit-learn} library. 
This kernel choice allows the decision boundary to be nonlinear, which is important given the heterogeneous mixture of categorical and numerical medical features in the dataset. 

\textbf{Loss Function:}
\[
\min_{\mathbf{w},b} \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(\mathbf{w}^\top \phi(\mathbf{x}_i) + b))
\]
where $C$ is the penalty parameter controlling the trade-off between the margin size and misclassification tolerance, and $\phi(\cdot)$ denotes the nonlinear mapping induced by the \textbf{RBF Kernel:}
\[
K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)
\]
The model was optimized using the \texttt{libsvm} implementation, which employs a coordinate descent solver with kernel caching for efficiency.

To evaluate our implementation, we compared SVM–RBF against two baseline models: 
\begin{itemize}
    \item \textbf{Logistic Regression:} This baseline model was chosen from kaggle \cite{musmanaslamawan_heartdisease_logistic}, where the author didn't use features selection or dimensionality reduction, and he was able to achieve 85\% accuracy.
    \item \textbf{Random Forest:} From this paper \cite{GARATEESCAMILA2020100330}, the author has 98\% accuracy by using Random Forest with Chi-PCA method. However, he used a 74 features and around 1000 datapoint of heartdiease dataset from UCL.
\end{itemize}

With SVM-RBF, we evaluate it's accuracy by using cross-validation, and we was only able to achieve 86\% of accuracy.

We then use Random Forest and Logistic Regression as our model, Randomforest was able to achieve 87\% accuracy, while Logistic Regression was able to achieve 85\% accuracy.

Varies reason can be introduces in here, such as different dataset, the quality of dataset, or minor changes in the preprocessing phase that effect the data values meaning.

In future iterations, we plan to explore a \textbf{neural network architecture} (e.g., a multi-layer perceptron) to capture more complex feature interactions and potentially improve generalization performance. 
We also intent to change the detail in our preprocessing phase, such as using different order, or different preprocessing tools to present the data in a better way.

% ============================================================
\section{Evaluation Strategy and Results}

\subsection{Evaluation Method}
Explain why you used stratified K-fold cross validation  
(e.g., small dataset size, need for robust evaluation).

\subsection{Metrics}
Explain why accuracy, precision, recall, F1, and AUC-ROC are important in medical diagnosis.

\subsection{Results}
Insert your figures:

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{kfoldvalidation.png}
\caption{5-fold precision scores.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{metrics_output.png}
\caption{Summary metrics across folds.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{AUC.png}
\caption{AUC–ROC curves.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{confuse_matrix.png}
\caption{Confusion matrices.}
\end{figure}

% ============================================================
\section{Feedback and Future Plans}
Summarize TA feedback and your improvements:
\begin{itemize}
    \item Replace label encoding with one-hot encoding
    \item Consider switching from SVM to neural networks for performance gains
    \item Create a new GitHub branch for experiments
\end{itemize}

% ============================================================
\section*{Team Contributions}
\textbf{Zhicong Lin:} Background Research, Data preprocessing, feature engineering, model implementation, report writing.
% ============================================================
% Bibliography
\bibliographystyle{plain}
\bibliography{custom}

\end{document}
